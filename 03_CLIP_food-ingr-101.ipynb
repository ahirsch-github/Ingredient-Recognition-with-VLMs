{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "```\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install pandas matplotlib transformers pillow wandb scikit-learn ftfy regex tqdm\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class FoodIngredientsDataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train', transform=None, mlb=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.data = self.load_data()\n",
    "        self.mlb = mlb\n",
    "        \n",
    "        if self.mlb is None:\n",
    "            self.mlb = MultiLabelBinarizer()\n",
    "            all_ingredients = [item['suffix'].split(', ') for item in self.data]\n",
    "            self.mlb.fit(all_ingredients)\n",
    "        \n",
    "        self.num_classes = len(self.mlb.classes_)\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(os.path.join(self.data_dir, f'{self.split}.jsonl'), 'r') as f:\n",
    "            return [json.loads(line) for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image_path = item['image']\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        ingredients = item['suffix'].split(', ')\n",
    "        labels = self.mlb.transform([ingredients])[0]\n",
    "        \n",
    "        return image, torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading function\n",
    "def load_data(data_dir, batch_size):\n",
    "    train_dataset = FoodIngredientsDataset(data_dir, 'train', transform=preprocess)\n",
    "    mlb = train_dataset.mlb\n",
    "    val_dataset = FoodIngredientsDataset(data_dir, 'test', transform=preprocess, mlb=mlb)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, mlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image_features = model.encode_image(images)\n",
    "        logits = model.visual.proj(image_features)\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            image_features = model.encode_image(images)\n",
    "            logits = model.visual.proj(image_features)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='samples')\n",
    "    recall = recall_score(all_labels, all_preds, average='samples')\n",
    "    f1 = f1_score(all_labels, all_preds, average='samples')\n",
    "\n",
    "    return total_loss / len(val_loader), accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "def run_training():\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 1e-5\n",
    "    EPOCHS = 1\n",
    "    DATA_DIR = './data/food-ingredients-101'\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"clip-finetuning\", config={\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": EPOCHS,\n",
    "    })\n",
    "\n",
    "    # Load data\n",
    "    train_loader, val_loader, mlb = load_data(DATA_DIR, BATCH_SIZE)\n",
    "\n",
    "    # Modify the model's projection layer to match the number of classes\n",
    "    num_classes = len(mlb.classes_)\n",
    "    \n",
    "    # Get the current output dimension of the visual projection\n",
    "    current_dim = model.visual.proj.shape[0]\n",
    "    \n",
    "    # Create a new linear layer\n",
    "    new_proj = torch.nn.Linear(current_dim, num_classes).to(device)\n",
    "    \n",
    "    # Initialize the weights of the new layer\n",
    "    torch.nn.init.normal_(new_proj.weight, std=0.001)\n",
    "    torch.nn.init.zeros_(new_proj.bias)\n",
    "    \n",
    "    # Replace the old projection layer with the new one\n",
    "    model.visual.proj = new_proj\n",
    "\n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        \n",
    "        train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, accuracy, precision, recall, f1 = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "        # Log to wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1\n",
    "        })\n",
    "\n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_loss,\n",
    "            'mlb': mlb,\n",
    "        }, f\"clip_checkpoint_epoch_{epoch+1}.pt\")\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mahirsch\u001b[0m (\u001b[33mhtwbe\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Dev\\Projects D\\Anja\\MA\\wandb\\run-20240702_140805-id4chpiv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/htwbe/clip-finetuning/runs/id4chpiv' target=\"_blank\">skilled-durian-5</a></strong> to <a href='https://wandb.ai/htwbe/clip-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/htwbe/clip-finetuning' target=\"_blank\">https://wandb.ai/htwbe/clip-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/htwbe/clip-finetuning/runs/id4chpiv' target=\"_blank\">https://wandb.ai/htwbe/clip-finetuning/runs/id4chpiv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.nn.modules.linear.Linear' as parameter 'proj' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mzeros_(new_proj\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Replace the old projection layer with the new one\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m \u001b[38;5;241m=\u001b[39m new_proj\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Initialize optimizer and loss function\u001b[39;00m\n\u001b[0;32m     36\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n",
      "File \u001b[1;32mc:\\Users\\sbene\\miniconda3\\envs\\anja-ma-clip\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1729\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   1728\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1729\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mtypename(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m as parameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1730\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(torch.nn.Parameter or None expected)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1731\u001b[0m                         )\n\u001b[0;32m   1732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(name, value)\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot assign 'torch.nn.modules.linear.Linear' as parameter 'proj' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anja-ma-clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
